\section{Evaluation Environment}
\label{sec:testingEnv}

%% 
%% Leave first page empty
\thispagestyle{empty}

In this chapter we will describe the testbed we use for running all the WebRTC tests, the environment is described in Figure~\ref{fig:evaluation_arch}. 

Some of the nodes carry specific software that enable some extra features or conditions that are required for deep testing.

 \begin{figure}[h]
  \centering
    \includegraphics[width=1\textwidth]{./figures/evaluation_arch.pdf}
      \caption[Description of simple testing environment topology for WebRTC]{Description of simple testing environment topology for WebRTC.}
	\label{fig:evaluation_arch}
\end{figure}

With the testbed described in Figure~\ref{fig:evaluation_arch} we are able to control the bottleneck link capacity, end-to-end latency, link loss rate and the queue size of the intermediate routers.

%By using this testbed we can investigate the following:
%
%\begin{itemize}
%	\item Utilization of bottleneck link capacity
%	\item (un)friendliness to parallel media or cross-traffic
%	\item Performance in multiparty calls
%\end{itemize}

\subsection{WebRTC client}

WebRTC clients are virtual machines that run a lightweight version of Ubuntu (Lubuntu\footnote{https://wiki.ubuntu.com/Lubuntu}) with 2GB of RAM and one CPU. This light version removes graphic acceleration providing better results in performance than compared with other distributions due to the virtualization of the graphic card. 

Clients run Chrome Dev version 27.01453.12 as a WebRTC capable browser. We have disabled audio in our tests in order to avoid unexpected performance results due to a bug in the {\it Pulse Audio} module that controls audio input in WebRTC\footnote{https://bugs.launchpad.net/ubuntu/+source/pulseaudio/+bug/1170313}. The amount of audio data transferred due to the echo cancelation systems can be neglected.

This client will load the web that runs on the Application Server to handle the WebRTC calls.

\subsubsection{Connection Monitor}

Connection Monitor [{\it ConMon}] is a command line utility that works on the transport layer and uses {\it libpcap}\footnote{http://www.tcpdump.org/} to sniff all the packets that go to a certain interface and port~\cite{singhConMon}. This utility is designed specifically to detect and capture RTP/UDP packets. {\it ConMon} detects and saves the header but discards the payload of each packet keeping the information we need for calculating our performance indicators.

Typically we run the {\it PeerConnection} between two devices and start capturing those packets using {\it ConMon} at each endpoint. The {\it PeerConnection} carries real media so the testing environment is a real scenario of WebRTC usage.

{\it ConMon} captures are be saved into different files allowing us to plot separately different stream rates and calculate other parameters such as delay with post processing. {\it ConMon} allows us to compare network layer and JavaScript API monitoring tools, as {\it ConMon} is working directly over the network interface and avoids all the processing that the browser internals do to send the stats to the JavaScript statistical API. Figure~\ref{fig:onetooneWifiRTCConMon} represents one video stream from the same call as Figure~\ref{fig:onetooneWifiRTC} captured from the {\it ConMon} application.

 \begin{figure}[h]
  \centering
    \includegraphics[width=1\textwidth]{./figures/onetooneWiFiConMon.pdf}
      \caption[Point-to-point WebRTC video stream throughput graph using ConMon at the network layer]{Point-to-point WebRTC video stream throughput graph using ConMon at the network layer.}
	\label{fig:onetooneWifiRTCConMon}
\end{figure}

The capture from {\it ConMon} is very accurate as it analyzes all the packets that go through an interface, this data is processed and averaged for each period of one second. 

%This process might output some fluctuations on the graph that could distort the reality in some cases..

Furthermore, {\it ConMon} is used to provide OWD and RTT calculations for our tests, in order to do this we assure a proper synchronization between local clocks in all the peers. This is done by using the sequence number of all RTP packets captured and subtracting the timestamp stored from both sides, no RTCP data is used in this analysis.

{\it ConMon} provides us with higher accuracy by subtracting the timestamps from both endpoints of the same stream, to obtain a good result in this test we have to reduce the internal clock drift of both peers using systems such as Network Time Protocol daemon (NTPd) \nomenclature{NTPd}{Network Time Protocol daemon}.

\subsubsection{Stats API}

WebRTC statistical API provides methods to help developers access the lower layer network information at the receiver, those methods return the information required to calculate the performance indicators used to build the high level JavaScript {\it Stats API}. When using those statistics we process all the output data to obtain the metrics for WebRTC.

This system works in parallel with {\it ConMon}, both of them can provide similar results of some metrics and the comparison of both it interesting in order to check the differences between the browser API and an interface layer capture. By combining both methods we can verify the results and accuracy of the metrics.

The method used for {\it Stats API} is the {\it RTCStatsCallback} that returns a JSON object that has to be parsed and manipulated to get the correct indicators, this object returns as many arrays as streams available in a {\it PeerConnection}, two audio and video~\cite{editorWebRTCdraft} objects per {\it PeerConnection} when having a point-to-point call. This data is provided by the lower layers of the network channel extracting the information from the RTCP packets that come multiplexed in the same network port~\cite{rtpusageIETF}.

{\it RTCStatsCallback} is the mechanism of WebRTC that allows the developer to access different metrics, as this is still in an ongoing discussion the stats report object has not been totally defined and can change slightly in the following versions of the WebRTC API. Methods involved in the {\it RTCStatsCallback} are available on the W3C editors draft~\cite{editorWebRTCdraft}. 

We have built a high level {\it Stats API} that uses those statistics from the {\it RTCStatsCallback} to calculate the RTT, throughput and loss rate for the different streams that are sent over the {\it PeerConnection} during the call. Those stats can be saved into a file at the end of the call for further process. Our JavaScript API grabs any {\it PeerConnection} passed through the variable and starts a periodic iteration to collect those stats and, either plot them or save them for post-processing. 

Figure~\ref{fig:onetooneWifiRTC} represents an example of a captured call between two browsers in two different machines, Mac and Ubuntu, the call was made over Wifi network with no firewall but with unknown cross traffic on it. The measures were directly obtained from the {\it Stats API} we built and post-processed using {\it gnuplot}\footnote{http://www.gnuplot.info/}.

 \begin{figure}[h]
  \centering
    \includegraphics[width=1\textwidth]{./figures/onetooneWifiStatsRTC.pdf}
      \caption[Point-to-point WebRTC video call total throughput graph using {\it Stats API} over public WiFi]{Point-to-point WebRTC video call total throughput graph using {\it Stats API} over public WiFi.}
	\label{fig:onetooneWifiRTC}
\end{figure}

Figure~\ref{fig:onetooneWifiRTC} plots the overall bandwidth of the call, this means that the input/output video and audio are measured together to check how much total bandwidth is being consumed in both directions over the duration of the call. As it is using RTCP packets to deliver the metrics to the {\it Stats API}, it takes a while to reach the average rate value until congestion mechanisms adapt the used rate to the network conditions. We can then plot all the different streams together to get an idea of how much bandwidth the {\it PeerConnection} is consuming.

% \begin{figure}[h]
%  \centering
%    \includegraphics[width=1\textwidth]{./figures/onetooneWiFIStatsVideoStreams.pdf}
%      \caption[Point-to-point WebRTC input/output video throughput graph using Stats API over public WiFi]{Point-to-point WebRTC input/output video throughput graph using Stats API over public WiFi.}
%	\label{fig:onetooneWifiRTCVideoStreams}
%\end{figure}
%
%Figure~\ref{fig:onetooneWifiRTCVideoStreams} shows the two video streams captured from the same machine, one is outgoing the local video stream meanwhile the second stream is the incoming video stream from the other peer. We have built a flexible processing system that allows us to capture and analyze all the possible combinations of streams and metrics. The timing used for the capture is provided by the TimeStamp available on the RTCP. The average bandwidth used in this scenario of point-to-point call in a standard wireless network is around 2000 Kbps per video stream. Both figures are plotted from the same original call.

% \begin{figure}[h]
%  \centering
%    \includegraphics[width=1\textwidth]{./figures/p2prttexample.pdf}
%      \caption[Point-to-point WebRTC RTT measure using Stats API over public WiFi]{Point-to-point WebRTC RTT measure using Stats API over public WiFi.}
%	\label{fig:p2prttexample}
%\end{figure}

%Our Stats API also provides extra information such as RTT and loss rate, RTT should be provided natively by the WebRTC %method but it is possible to calculate it by using the DataChannel provided by the PeerConnection, we are using this %channel to send a UNIX TimeStamp object to the other peer and take it back, when the round trip is finished we compare it %with the actual millisecond and obtain the total RTT.

%Figure~\ref{fig:p2prttexample} represents the capture of a video call between two peers, if we are able to process the JavaScript forwarding function in an optimal way this would led to a precise RTT measurement hack without the need of Stats method.

\subsubsection{Analysis of tools}

{\it Stats API} and {\it ConMon} measure the same metrics but at different layers of the operating system, this provides us with some extra information in order to see how the high level {\it Stats API} works and if it is reliable and accurate.

However, due to the periodical capture method, the output can produce strange plots as the information regarding to the next data period could be stored in the previous one when processing the averaged data on the system. This is an accuracy problem that cannot be easily solved, when looking at the graph, it is important to observe if two peaks (positive and negative) get compensated by each other, this would mean that the data has not been allocated to the correct period when plotted. 

%This accuracy error is a problem that can be observed when comparing both {\it ConMon} and {\it Stats API} capture in Figure~\ref{fig:delay_mesh_peer1}. 

A second problem that we could face is the time it takes for the OS to process the stats from the RTCP packet and send them to the upper browser layer, at the receiver some of the stats are based on the current measurement of the metrics not in the RTP Receiver Report. Figure~\ref{fig:p2pincommingStatsConmonWifi} and~\ref{fig:p2poutgoingStatsConmonWifi} plot two video streams being captured from Stats API and {\it ConMon}.

\begin{figure}[h]
        \centering
        \begin{subfigure}[b]{1\textwidth}
                \centering
                \includegraphics[width=\textwidth]{./figures/p2p_incomming_cable_sample.pdf}
               \caption[Incomming stream]{Incomming stream.}
			\label{fig:p2pincommingStatsConmonWifi}
        \end{subfigure}
        \begin{subfigure}[b]{1\textwidth}
                \centering
                \includegraphics[width=\textwidth]{./figures/p2p_outgoing_cable_sample.pdf}
               \caption[Outgoing stream]{Outgoing stream.}
			\label{fig:p2poutgoingStatsConmonWifi}
	        \end{subfigure}
        \caption[P2P video stream comparison between {\it ConMon} and {\it Stats API}]{P2P video stream comparison between {\it ConMon} and {\it Stats API}.}
        \label{fig:p2pStatsConmon}
\end{figure}

Figure~\ref{fig:p2pStatsConmon} represents the incoming media stream from the other peer, we can see the little overhead that is not captured by the {\it Stats API} interface, as it just reads the bytes inside the payload of the packet. All the overhead is not considered when calculating the rate through {\it Stats API}. We can conclude that the real rate that WebRTC use is the one outputted by the result of {\it ConMon} instead of {\it Stats API}, but {\it Stats API} is going to be an accurate approach.

\subsubsection{Automated testing}

For our test scenario we considered two options, manual and automated testing. The first test environment does not give as much accuracy due to the difficulty to iterate the test many times for the same configuration, if the second option is available the results can be averaged between all the iterations resulting in an accurate output. We try to automate the process in most of the tests.

In some environments, we won't be able to perform automated testing, when this happens the averaged results after the iterations will not show a leveraged value that defines the behavior. This help us to deal with outliers that can provide false values due unexpected conditions.

One of the main issues when building a test scenario is the media provided to the {\it GetUserMedia} input, this media must be as close to reality as possible without using a real webcam. Google Chrome provides a fake video flag that can be activated by adding {\it --use-fake-device-for-media-stream}\footnote{http://peter.sh/experiments/chromium-command-line-switches/} parameter, this video though, does not produce rate high enough for our purposes.

%\begin{figure}[h]
%	\begin{minipage}{.5\textwidth}
%		\includegraphics[width=1\textwidth]{./figures/realVideoChrome.pdf}
%			\caption[Video stream bandwidth between two peers using webcam]{Video call bandwidth between two peers using webcam.}
%			\label{fig:realVideoChrome}
%	 \end{minipage}
%	 \begin{minipage}{.5\textwidth}
%		\includegraphics[width=1\textwidth]{./figures/automatedVideoChrome.pdf}
%			\caption[Video stream bandwidth between two peers using fake video]{Video stream bandwidth between two peers using fake video.}
%			\label{fig:automatedVideoChrome}
%	 \end{minipage}
%\end{figure}

 \begin{figure}[h]
  \centering
   \includegraphics[width=1\textwidth]{./figures/realVideoChrome.pdf}
     \caption[Video stream rate with SSRC 0x646227 captured using {\it Stats API} and webcam input]{Video stream rate with SSRC 0x646227 captured using {\it Stats API} and webcam input.}
	\label{fig:realVideoChrome}
%\end{figure}
 %\begin{figure}[h]
  \centering
	\includegraphics[width=1\textwidth]{./figures/automatedVideoChrome.pdf}
	\caption[Video stream rate with SSRC 0x3a4df354 captured using {\it Stats API} and Chrome default fake content]{Video stream rate with SSRC 0x3a4df354 captured using {\it Stats API} and Chrome default fake video input.}
	\label{fig:automatedVideoChrome}
\end{figure}

Figure~\ref{fig:realVideoChrome} represents the approximate bandwidth that a real video call uses when receives media from another peer, that capture shows the same stream captured from the receiver {\it StatsAPI} perspective. The maximum rate increases up to 2000 Kbps. On the other hand, Figure~\ref{fig:automatedVideoChrome} represents the scenario but using the built-in fake video in both clients, the rate for this case drops to an average of 250 Kbps. 

Both figures (\ref{fig:realVideoChrome} and~\ref{fig:automatedVideoChrome}) show one single video stream, identified with the Synchronization Source identifier (SSRC) \nomenclature{SSRC}{Synchronization Source identifier}, but from the sender and receiver perspective, {\it LV} identifies the source capture and {\it RV} the receiver stream rate. 

Comparing global output from Figures ~\ref{fig:realVideoChrome} and ~\ref{fig:automatedVideoChrome}, we can see that the obtained rate is very different concluding that we cannot use {\it --use-fake-device-for-media-stream} flag for our testing environment. The reason is that Google Chrome uses an internal bitmap engine to draw the figures that are rendered in the video tag and sent over the {\it PeerConnection}, this means that the amount of encoding and bandwidth used will be low compared to a real webcam as the media sent over with fake video is not representative.

To address this issue of the media streaming for our automated devices, we have built a fake input device on the peers, the procedure is described in Appendix~\ref{sec:fakeVideo}. This technique utilizes a modified version of the {\it V4L2loopback} linux driver to create two extra video devices that play a selected YUV file.  

 \begin{figure}[h]
  \centering
    \includegraphics[width=1\textwidth]{./figures/testV4L2niklas.pdf}
      \caption[Video stream bandwidth using V4L2Loopback fake YUV file]{Video stream bandwidth using V4L2Loopback fake YUV file.}
	\label{fig:testV4L2niklas}
\end{figure}

Figure~\ref{fig:testV4L2niklas} represents the bandwidth of a fake video stream measured by our {\it Stats API} using an YUV\footnote{YUV is a color space that encodes video taking human perception into account, typically enabling transmission errors or compression artifacts to be more efficiently masked by the human perception than using RGB-representation.} video captured from a Logitech HD Pro C910 as source, resolution is 640x480 at a frame-rate of 30 fps. 

Results can be compared between Figure~\ref{fig:testV4L2niklas} and~\ref{fig:realVideoChrome}, both average rate output is approximately 2000 Kbps, which means that this procedure is a good approach to a real webcam. 

The combination of the previous fake video setup and multiple Secure Shell \nomenclature{SSH}{Secure Shell} scripts enables the automation mechanisms to run multiple tests without the need of multiple physical devices.

\subsection{TURN Server}

Our TURN server is used to pipe all the media as a relay, allowing us to apply the network constraints required for the tests to a centralized node, this machine is a Ubuntu Server 12.04 LTS with a tuned kernel adapted to perform better with {\it Dummynet}.

The TURN daemon we use is called {\it Restund}, which has been proven to be reliable for our needs, this open source STUN/TURN server works with {\it MySQL} database authentication~\cite{restund}. We have modified the source in order to have a hardcoded password making it easier for our needs.

To do so, we need to modify {\it db.c} file before compiling. Content of  method {\it restund\_get\_ha1} has to be replaced with the following line of code, where XXX is username and YYY the password we use for the TURN configuration.

\lstset{language=C}
\begin{lstlisting}[caption=Forcing a hardcoded password in our TURN server]
md5_printf(ha1, "\%s:\%s:\%s", "XXX", "myrealm", "YYY");
\end{lstlisting}

Furthermore, in order to force WebRTC to use TURN candidates we need to replace the WebRTC API server identification with our TURN machine by doing:

\lstset{language=JavaScript}
\begin{lstlisting}[caption=Configuring our TURN server in WebRTC,label={lst:listingICE}]
var pc_config = {
	"iceServers": [{url: "turn:XXX@192.168.1.106:3478", credential:"YYY"}]
};
\end{lstlisting}

Listing~\ref{lst:listingICE} object is provided to the {\it PeerConnection} object enabling the use of TURN.

The IP address points to our TURN server and the desired port (3478 by default), now all candidates are obtained through our TURN. This does not mean that the connection will run through the relay as WebRTC will try to find the best path which may override TURN, to force the usage of TURN candidates we need to drop all candidates that do not force the use of the relay.

\lstset{language=JavaScript}
\begin{lstlisting}[caption=Dropping all candidates except relay]
function onIceCandidate(event) {
  if ((event.candidate) && (event.candidate.candidate.toLowerCase().indexOf('relay')) !== -1) {
		sendMessage({
               		type: 'candidate',
               		label: event.candidate.sdpMLineIndex,
               		id: event.candidate.sdpMid,
               		candidate: event.candidate.candidate
          	 },receiver,from);
  } else {
           	console.log("End of candidates.");
  }
}
\end{lstlisting}

Function {\it onIceCandidate} is called every time we get a new candidate from our STUN/TURN or WebRTC API, those candidates need to be forwarded to the other peer by using our own method {\it sendMessage} through {\it WebSockets} or similar polling methods. In this code, we are dropping all candidates except the ones containing the option {\it relay} on it, those are the candidates that force the {\it PeerConnection} to go through our TURN machine.

This part is important as it allow us to set the constraints in a middle point without affecting the WebRTC peers.

\subsubsection{Dummynet}

To evaluate the performance of WebRTC we may modify the conditions of the network path to imitate some specific environments. This is achieved using {\it Dummynet}, a command line network simulator that allows us to add bandwidth limitations, delays, packet losses and other distortions to the ongoing link~\cite{dummynetTool}.

{\it Dummynet} is a standard tool for some Linux distributions and OSX~\cite{dummynetTool}. In order to get appropriate results we need to apply the {\it Dummynet} rules in the TURN server, this machine will forward all the WebRTC traffic from one peer to the other being transparent for both ends. 

The real goal of using TURN in WebRTC is to bypass some restrictive Firewalls that could block the connection, in our case, this works as a way to centralize the traffic flow through one unique path that we can monitor and modify. From the performance perspective, when not adding any rules to the TURN, the traffic and response of WebRTC is normal without the user noticing any difference.

Some problems arise when using {\it Dummynet} in our scenario, we will use {\it VirtualBox}\footnote{VirtualBox is an x86 virtualization software package.} machines for some testing and for running TURN instance, read Appendix~\ref{sec:dummynet} for the fixes in {\it Dummynet} configuration for virtual machines.

\subsection{Application Server}

Our application server runs the Node.js~\footnote{http://nodejs.org/} instance to handle the WebRTC signaling part, this machine uses Ubuntu with a domain name specified as {\it dialogue.io}. 

This app is a common working group application that allows people to chat and video call at the same time in their own private chat rooms, we have modified it to build a specific room for our tests, this instance simply allows two users that access the page to automatically call each other and start running the JavaScript code with the built-in {\it Stats API}.

Most of this application is coded using JavaScript APIs and uses WebSocket~\footnote{http://socket.io/} protocol to handle the signaling messages from peer to peer. 

The web browsers in our clients establish an HTTPS connection between the Application Server to download the web page that contain the {\it call.js} file that executes the WebRTC features.

\subsection{Summary of tools}

Using all the previous mentioned tools together we are able to measure how WebRTC performs in a real environment, similar to Figure~\ref{fig:evaluation_arch}, some tools have been modified according to our requirements of bandwidth and security. To process the data obtained by all those tools we use some special scripts that measure and extract the information we require from the captures, some of them are explained in Appendix~\ref{sec:scriptsWebRTC}.